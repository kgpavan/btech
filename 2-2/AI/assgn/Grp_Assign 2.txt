Assignment HA-N2
	TBL TRAINING AND TESTING FOR CHUNKING
					(Due 22 Jan, 07)

Chunking is the task of dividing the sentence into syntactically corelated
parts of words. Common chunk tags are NP, VP, PP. 

[NP Children/NNS ] (VP are/VBP watching/VBG ) [NP some/DT programmes/NNS]
[PP on/IN] [NP telivision/NN] [PP in/IN] [NP the/DT house/NN]

Text chunking is an intermediate step towards full parsing.

The goal of this task is to implement a transformation based learning system which
after a training phase can recognize the chunk segmentation of the test data
as well as possible. The training data can be used for training the text
chunker. 

The train and test data consist of three columns separated by spaces. Each
word has been put on a separate line and there is an empty line after each
sentence. The first column contains the current word, the second its
part-of-speech tag as derived by the Brills tagger (which is a TBL based tagger)
and the third its chunk tag as derived from the WSJ corpus. The chunk tags 
contain the name of the chunk type, for example I-NP for noun phrase words 
and I-VP for verb phrase words. Most chunk types have two types of chunk 
tags, B-CHUNK for the first word of the chunk and I-CHUNK for each other 
word in the chunk. Here is an example of the file format:

	Children	NNS	B-NP
	are		VBP	B-VP
	watching	VBG	I-VP
	some		DT	B-NP
	programmes	NNS	I-NP
	on		IN	B-PP
	telivision	NN	B-NP
	in		IN	B-PP
	the		DT	B-NP
	house		NN	I-NP
	.		.	O

The O chunk tag is used for tokens which are not part of any chunk. Instead of
using the part-of-speech tags of the WSJ corpus, the data set used tags
generated by the Brill tagger (you can even use tags given the TnT tagger from
the first assignment).

As discussed in the class, the first step in building a TBL based system is to 
come up with an initial chunker. A simple initial chunker can pick the most 
frequent chunk tag for a word as its chunk. For unknown words (i.e for words
that aren't present in the train data), give 'O' as its chunk tag.

Now, give a set of templates. The standard notation for a template can be



w0 - current word
t0 - current word's POS tag
c0 - current word's chunk tag
tc0 - current word's true chunk tag (from the training data)

w-1	previous word
t-1	previous word's POS tag
c-1	previous word's chunk tag

w-2	2nd word to the left of the current word
t-2	w-2's POS tag
c-2	w-2's chunk tag

w+1	next word
t+1	next word's POS tag
c+1	next word's chunk tag

w+2	2nd word to the right of the current word
t+2	w+2's POS tag
c+2	w+2's chunk tag



So, an example template can have the following syntax:-
tag	tag	context

For ex:-

c0	tc0	c-1	c-2

The above template says, change 'c0' to 'tc0' if c-1 equals something and c-2
equals something. This template takes the previous two POS tags as the
context. This template may generates rules such as

B-NP	I-NP	c-1=B-NP	c-2=I-VP

which says change B-NP to I-NP if c-1=B-NP and c-2=I-VP.

Some example templates are:-
c0	tc0	c-1	c-2	(chunk tags of prev two words)
c0	tc0	c+1	c+2	(chunk tags of next two words)
c0	tc0	c-1	t-1	(chunk tag and POS tag of prev word)
c0	tc0	c+1	t+1	(chunk tag and POS tag of next word)
c0	tc0	t-1	w-1	(POS tag and lexeme of prev word )
c0	tc0	t+1	w+1	(POS tag and lexeme of next word )
c0	tc0	t-1		(POS tag of prev word)
c0	tc0	w-1		(prev word)
c0	tc0	t-1	t-2	(POS tags of prev two words)
c0	tc0	t+1		(POS tag of next word)
c0	tc0	w+1		(next word)
c0	tc0	t+1	t+2	(POS tags of next two words)
.
.
.
.

You can experiment with different templates. To start with use a small set of
templates. 
After the training is done, use only the first two columns in the test data set to
fill the third column with chunk tags from the inital chunker. Use the rules learnt while
training to correct the tags predicted by the initial tagger.

So, you need to implement the following programs:-
1. Initial chunker (a simple program)
2. Training system which can read templates from a file and come up with
a set of rules to correct the output of the initial tagger.
3. Testing system which uses these set of rules and corrects the 
output of the initial chunker on the test data.

The code can be in any one of the langagues (C, C++, perl, python, Java).
Report your accuracies and the templates used in a one page README file.

This is not a trivial assignment. Its a challenging assignment. Hope you are
up for it. 

Train data can be downloaded from 
http://www.cnts.ua.ac.be/conll2000/chunking/train.txt.gz

Test data can be downloaded from
http://www.cnts.ua.ac.be/conll2000/chunking/test.txt.gz

I'm attaching a paper by Ramshaw and Marcus on "Text Chunking using TBL". 
Its a wonderful guide for TBL as well as Chunking. You'll get lots of insight
by reading it.

References:-

CoNLL-2001 Shared Task on Chunking
http://www.cnts.ua.ac.be/conll2000/chunking/
(Lots of information of chunking and various other ML methods)

Lance A. Ramshaw and Mitchell P. Marcus, Text Chunking Using
Transformation-Based Learning. In: "Proceedings of the Third ACL Workshop on
Very Large Corpora", Cambridge MA, USA, 1995.
ftp://ftp.cis.upenn.edu/pub/chunker/wvlcbook.ps.gz


Grace Ngai and Radu Florian. Transformation Based Learning in the Fast Lane.
In: "Proceedings of NAACL 2001", Pittsburgh, PA, USA, 2001.
http://nlp.cs.jhu.edu/~rflorian/papers/naacl01.ps

Google "transformation based learning " for more.